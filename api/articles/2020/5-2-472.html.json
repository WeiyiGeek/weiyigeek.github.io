{"title":"n0-Kubernetes入坑解决FAQ记录","slug":"虚拟云容/云容器/Kubernetes/n0-Kubernetes配置解析与入坑解决FAQ记录","date":"2020-05-02T09:37:47.000Z","updated":"2022-09-29T05:42:15.007Z","url":"2020/5-2-472.html","path":"api/articles/2020/5-2-472.html.json","covers":["https://cdn.jsdelivr.net/gh/WeiyiGeek/blogimage/2020/1/20200507094655.png"],"content":"<p>[TOC]</p>\n<a id=\"more\"></a>\n<h2 id=\"0x00-简述\"><a href=\"#0x00-简述\" class=\"headerlink\" title=\"0x00 简述\"></a>0x00 简述</h2><p>描述:在学习任何一门新技术总是免不了坑坑拌拌，当您学会了记录坑后然后将其记录当下次遇到，相同问题的时候可以第一时间进行处理;</p>\n<hr>\n<h3 id=\"0x01-配置文件与启动参数\"><a href=\"#0x01-配置文件与启动参数\" class=\"headerlink\" title=\"0x01 配置文件与启动参数\"></a>0x01 配置文件与启动参数</h3><h3 id=\"1-Kubelet-启动参数\"><a href=\"#1-Kubelet-启动参数\" class=\"headerlink\" title=\"1.Kubelet 启动参数\"></a>1.Kubelet 启动参数</h3><p>启动参数总结一览表:<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">--register-node [Boolean] <span class=\"comment\"># 节点是否自动注册</span></span><br></pre></td></tr></table></figure></p>\n<p>/etc/kubernetes/kubelet.conf</p>\n<p>关于构建环境</p>\n<p>您可以根据自己的情况将构建环境与部署环境分开，例如：</p>\n<p>学习时，参考本教程，使用 kubernetes 的 master 节点完成 构建和镜像推送<br>开发时，在自己的笔记本上完成 构建和镜像推送<br>工作中，使用 Jenkins Pipeline 或者 gitlab-runner Pipeline 来完成 构建和镜像推送</p>\n<p><br/></p>\n<p><strong>K8S Containerd 镜像回收GC参数配置</strong><br>参考地址: <a href=\"https://kubernetes.io/docs/concepts/architecture/garbage-collection/\" target=\"_blank\" rel=\"noopener\">https://kubernetes.io/docs/concepts/architecture/garbage-collection/</a><br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ vim /var/lib/kubelet/config.yaml</span><br><span class=\"line\">imageGCHighThresholdPercent: 85</span><br><span class=\"line\">imageGCLowThresholdPercent: 80</span><br><span class=\"line\">maxPods: 180 <span class=\"comment\"># pod最大数</span></span><br></pre></td></tr></table></figure></p>\n<p><strong>Kubelet 相关配置只要修改后,都需进入如下操作</strong><br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">systemctl daemon-reload</span><br><span class=\"line\">systemctl restart kubelet.service</span><br></pre></td></tr></table></figure></p>\n<hr>\n<h2 id=\"0x02-入坑弃坑\"><a href=\"#0x02-入坑弃坑\" class=\"headerlink\" title=\"0x02 入坑弃坑\"></a>0x02 入坑弃坑</h2><h3 id=\"问题1-初始化master节点镜像拉取失败问题\"><a href=\"#问题1-初始化master节点镜像拉取失败问题\" class=\"headerlink\" title=\"问题1.初始化master节点镜像拉取失败问题\"></a>问题1.初始化master节点镜像拉取失败问题</h3><p>描述:APISERVER_NAME 不能是 master 的 hostname，且必须全为小写字母、数字、小数点，不能包含减号<code>export APISERVER_NAME=apiserver.weiyi</code>；<br>POD_SUBNET 所使用的网段不能与 master节点/worker节点 所在的网段重叠( <code>CIDR 值:无类别域间路由，Classless Inter-Domain Routing</code>),<code>export POD_SUBNET=10.100.0.1/16</code>。<br>解决办法:<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 1.如不能下载 kubernetes 的 docker 镜像 ，请替换镜像源以及手工初始化</span></span><br><span class=\"line\"><span class=\"comment\"># --image-repository= mirrorgcrio</span></span><br><span class=\"line\"><span class=\"comment\"># --image-repository=registry.cn-hangzhou.aliyuncs.com/google_containers</span></span><br><span class=\"line\">~$ kubeadm config images list --image-repository=registry.cn-hangzhou.aliyuncs.com/google_containers</span><br><span class=\"line\">registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v1.19.6</span><br><span class=\"line\">registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager:v1.19.6</span><br><span class=\"line\">registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler:v1.19.6</span><br><span class=\"line\">registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:v1.19.6</span><br><span class=\"line\">registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.2</span><br><span class=\"line\">registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.4.13-0</span><br><span class=\"line\">registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:1.7.0</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#2.检查环境变量</span></span><br><span class=\"line\"><span class=\"built_in\">echo</span> MASTER_IP=<span class=\"variable\">$&#123;MASTER_IP&#125;</span> &amp;&amp; <span class=\"built_in\">echo</span> APISERVER_NAME=<span class=\"variable\">$&#123;APISERVER_NAME&#125;</span> &amp;&amp; <span class=\"built_in\">echo</span> POD_SUBNET=<span class=\"variable\">$&#123;POD_SUBNET&#125;</span></span><br></pre></td></tr></table></figure></p>\n<p>Tips : 在重新初始化 master 节点前，请先执行 <code>kubeadm reset -f</code> 操作;</p>\n<p><br></p>\n<h3 id=\"问题2-Master与pod状态查看显示Pending-ImagePullBackoff-异常问题\"><a href=\"#问题2-Master与pod状态查看显示Pending-ImagePullBackoff-异常问题\" class=\"headerlink\" title=\"问题2.Master与pod状态查看显示Pending[ImagePullBackoff]异常问题\"></a>问题2.Master与pod状态查看显示<code>Pending[ImagePullBackoff]</code>异常问题</h3><p>问题描述:</p>\n<ul>\n<li>1.如果输出结果中出现 ImagePullBackoff 或者长时间处于 Pending 的情况<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"variable\">$kubectl</span> get pods calico-node-4vql2 -n kube-system -o wide</span><br><span class=\"line\">NAME                READY   STATUS    RESTARTS   AGE     IP       NODE   NOMINATED NODE   READINESS GATES</span><br><span class=\"line\">calico-node-4vql2   0/1     Pending[ImagePullBackoff ]   0          7m22s   &lt;none&gt;   node   &lt;none&gt;           &lt;none&gt;</span><br><span class=\"line\">NAME                                          READY   STATUS              RESTARTS   AGE</span><br><span class=\"line\">coredns-94d74667-6dj45                        1/1     ImagePullBackOff    0          12m</span><br><span class=\"line\">calico-node-4vql2                             1/1     Pending             0          12m</span><br></pre></td></tr></table></figure>\n解决方法:<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#（1）通过get pods找到pod被调度到了哪一个节点并，确定 Pod 所使用的容器镜像：</span></span><br><span class=\"line\">kubectl get pods calico-node-4vql2 -n kube-system -o  yaml | grep image:</span><br><span class=\"line\">- image: calico/node:v3.13.1</span><br><span class=\"line\">- image: calico/cni:v3.13.1</span><br><span class=\"line\">- image: calico/pod2daemon-flexvol:v3.13.1</span><br><span class=\"line\"></span><br><span class=\"line\">kubectl get pods coredns-94d74667-6dj45 -n kube-system -o yaml | grep image:</span><br><span class=\"line\">- image: registry.aliyuncs.com/google_containers/coredns:1.3.1</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#（2）在 Pod 所在节点执行 docker pull 指令(当Node状态为NotReady时候也可以采用此种方法，但不是唯一)d</span></span><br><span class=\"line\">docker pull calico/node:v3.13.1</span><br><span class=\"line\">docker pull calico/cni:v3.13.1</span><br><span class=\"line\">docker pull calico/pod2daemon-flexvol:v3.13.1</span><br><span class=\"line\"></span><br><span class=\"line\">docker pull registry.aliyuncs.com/google_containers/coredns:1.3.1</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#（3）然后在master节点上查看状态恢复正常</span></span><br><span class=\"line\">NAME                                       READY   STATUS    RESTARTS   AGE   IP              NODE   NOMINATED NODE   READINESS GATES</span><br><span class=\"line\">calico-node-4vql2                          1/1     Running   0          36m   10.10.107.192   node   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<figure class=\"image-box\">\n                <img src=\"https://cdn.jsdelivr.net/gh/WeiyiGeek/blogimage/2020/1/20200507094655.png\" alt=\"WeiyiGeek.Pending\" title=\"\" class=\"\">\n                <p>WeiyiGeek.Pending</p>\n            </figure>\n<ul>\n<li>2.输出结果中某个 Pod 长期处于 <code>ContainerCreating、PodInitializing 或 Init:0/3</code> 的状态：<br>解决办法:<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#（1）查看该 Pod 的状态</span></span><br><span class=\"line\">kubectl describe pods -n kube-system calico-node-4vql2</span><br><span class=\"line\">kubectl describe pods -n kube-system coredns-8567978547-bmd9f</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#（2）如果输出结果中，最后一行显示的是 Pulling image，请耐心等待</span></span><br><span class=\"line\">Normal  Pulling   44s   kubelet, k8s-worker-02  Pulling image <span class=\"string\">\"calico/pod2daemon-flexvol:v3.13.1\"</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#（3）将该 Pod 删除，系统会自动重建一个新的 Pod</span></span><br><span class=\"line\">kubectl delete pod kube-flannel-ds-amd64-8l25c -n kube-system</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p><br></p>\n<h3 id=\"问题3-worker节点-join加入cluster集群不成功的几种情况\"><a href=\"#问题3-worker节点-join加入cluster集群不成功的几种情况\" class=\"headerlink\" title=\"问题3.worker节点 join加入cluster集群不成功的几种情况\"></a>问题3.worker节点 join加入cluster集群不成功的几种情况</h3><ul>\n<li>1.#worker 节点不能访问 apiserver<ul>\n<li>如果 master 节点能够访问 apiserver、而 worker 节点不能，则请检查自己的网络设置，<code>/etc/hosts 是否正确设置？ 是否有安全组或防火墙的限制？</code><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#master节点验证</span></span><br><span class=\"line\">curl -ik https://localhost:6443</span><br><span class=\"line\"><span class=\"comment\">#worker节点验证</span></span><br><span class=\"line\">curl -ik https://apiserver.weiyi:6443</span><br><span class=\"line\"><span class=\"comment\">#正常输出结果如下所示：</span></span><br><span class=\"line\">HTTP/1.1 403 Forbidden</span><br><span class=\"line\">Cache-Control: no-cache, private</span><br><span class=\"line\">Content-Type: application/json</span><br><span class=\"line\">X-Content-Type-Options: nosniff</span><br><span class=\"line\">Date: Fri, 15 Nov 2019 04:34:40 GMT</span><br><span class=\"line\">Content-Length: 233</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  <span class=\"string\">\"kind\"</span>: <span class=\"string\">\"Status\"</span>,</span><br><span class=\"line\">  <span class=\"string\">\"apiVersion\"</span>: <span class=\"string\">\"v1\"</span>,</span><br><span class=\"line\">  <span class=\"string\">\"metadata\"</span>: &#123;</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure></li>\n</ul>\n</li>\n<li>2.#worker 节点默认网卡<ul>\n<li>Kubelet使用的 IP 地址 与 master 节点可互通（无需 NAT 映射），且没有防火墙、安全组隔离 </li>\n</ul>\n</li>\n<li>3.#master 节点生成的token已过有效时间为 2 个小时 <code>kubeadm token create</code></li>\n</ul>\n<p><br/></p>\n<h3 id=\"问题4-在master节点上执行kubectl命令报错-localhost-8080-was-refused\"><a href=\"#问题4-在master节点上执行kubectl命令报错-localhost-8080-was-refused\" class=\"headerlink\" title=\"问题4.在master节点上执行kubectl命令报错`localhost:8080 was refused\"></a>问题4.在master节点上执行kubectl命令报错`localhost:8080 was refused</h3><p>错误信息:<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">kubectl apply -f calico-3.13.1.yaml</span><br><span class=\"line\">The connection to the server localhost:8080 was refused - did you specify the right host or port?</span><br></pre></td></tr></table></figure><br>错误原因: 由于在初始化之后没将k8s的<code>/etc/kubernetes/admin.conf</code>拷贝到用户的加目录之中<code>/root/.kube/config</code><br>解决办法:<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># (1) 普通用户对集群访问配置文件设置</span></span><br><span class=\"line\">mkdir -p <span class=\"variable\">$HOME</span>/.kube</span><br><span class=\"line\">sudo cp -i /etc/kubernetes/admin.conf <span class=\"variable\">$HOME</span>/.kube/config</span><br><span class=\"line\">sudo chown $(id -u):$(id -g) <span class=\"variable\">$HOME</span>/.kube/config</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># (2) 自动运行设置 KUBECONFIG 环境以及k8s命令自动补齐</span></span><br><span class=\"line\">grep <span class=\"string\">\"export KUBECONFIG\"</span> ~/.profile | <span class=\"built_in\">echo</span> <span class=\"string\">\"export KUBECONFIG=<span class=\"variable\">$HOME</span>/.kube/config\"</span> &gt;&gt; ~/.profile</span><br><span class=\"line\">tee -a ~/.profile &lt;&lt;<span class=\"string\">'EOF'</span></span><br><span class=\"line\"><span class=\"built_in\">source</span> &lt;(kubectl completion bash)</span><br><span class=\"line\"><span class=\"built_in\">source</span> &lt;(kubeadm completion bash)</span><br><span class=\"line\"><span class=\"comment\"># source &lt;(helm completion bash)</span></span><br><span class=\"line\">EOF</span><br><span class=\"line\"><span class=\"built_in\">source</span> ~/.profile</span><br></pre></td></tr></table></figure></p>\n<p>PS : 如果在加入 k8s 集群时采用普通需要在前面加<code>sudo kubeadm init ...</code>用以提升权限否则将出现<code>[ERROR IsPrivilegedUser]: user is not running as root</code>该错误;</p>\n<p><br/></p>\n<h3 id=\"问题5-安装K8s时候kubelet报错提示-Container-runtime-network-not-ready\"><a href=\"#问题5-安装K8s时候kubelet报错提示-Container-runtime-network-not-ready\" class=\"headerlink\" title=\"问题5.安装K8s时候kubelet报错提示`Container runtime network not ready\"></a>问题5.安装K8s时候kubelet报错提示`Container runtime network not ready</h3><p>错误信息:<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">systemctl status kubelet</span><br><span class=\"line\">6月 23 09:04:02 master-01 kubelet[8085]: E0623 09:04:02.186893    8085 kubelet.go:2187] Container runtime network not ready: NetworkReady=<span class=\"literal\">false</span> reason:NetworkPluginNotReady mes...ninitialized</span><br><span class=\"line\">6月 23 09:04:04 master-01 kubelet[8085]: W0623 09:04:04.938700    8085 cni.go:237] Unable to update cni config: no networks found <span class=\"keyword\">in</span> /etc/cni/net.d</span><br></pre></td></tr></table></figure><br>问题原因: 由于master节点初始化安装后报错，在未进行重置的情况下又进行初始化操作或者重置操作不完整导致,还有一种情况是没有安装网络组件比如(<code>flannel 或者 calico</code>);<br>解决办法: 执行以下命令重置初始化信息，然后在重新初始化;<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">systemctl stop kubelet</span><br><span class=\"line\">docker stop $(docker ps -aq)</span><br><span class=\"line\">docker rm -f $(docker ps -aq)</span><br><span class=\"line\">systemctl stop docker</span><br><span class=\"line\">kubeadm reset</span><br><span class=\"line\">rm -rf <span class=\"variable\">$HOME</span>/.kube /etc/kubernetes</span><br><span class=\"line\">rm -rf /var/lib/cni/ /etc/cni/ /var/lib/kubelet/* </span><br><span class=\"line\">iptables -F &amp;&amp; iptables -t nat -F &amp;&amp; iptables -t mangle -F &amp;&amp; iptables -X</span><br><span class=\"line\">systemctl start docker</span><br><span class=\"line\">systemctl start kubelet</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#安装 calico 网络插件(没有高可用)</span></span><br><span class=\"line\">rm -f calico-3.13.1.yaml</span><br><span class=\"line\">wget -L https://kuboard.cn/install-script/calico/calico-3.13.1.yaml</span><br><span class=\"line\">kubectl apply -f calico-3.13.1.yaml</span><br></pre></td></tr></table></figure></p>\n<p><br/></p>\n<h3 id=\"问题6-执行kubeadm-reset无法进行节点重置，提示retrying-of-unary-invoker-failed\"><a href=\"#问题6-执行kubeadm-reset无法进行节点重置，提示retrying-of-unary-invoker-failed\" class=\"headerlink\" title=\"问题6.执行kubeadm reset无法进行节点重置，提示retrying of unary invoker failed\"></a>问题6.执行<code>kubeadm reset</code>无法进行节点重置，提示<code>retrying of unary invoker failed</code></h3><p>错误信息:<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[reset] Removing info <span class=\"keyword\">for</span> node <span class=\"string\">\"master-01\"</span> from the ConfigMap <span class=\"string\">\"kubeadm-config\"</span> <span class=\"keyword\">in</span> the <span class=\"string\">\"kube-system\"</span> Namespace</span><br><span class=\"line\">&#123;<span class=\"string\">\"level\"</span>:<span class=\"string\">\"warn\"</span>,<span class=\"string\">\"ts\"</span>:<span class=\"string\">\"2020-06-23T09:10:30.074+0800\"</span>,<span class=\"string\">\"caller\"</span>:<span class=\"string\">\"clientv3/retry_interceptor.go:61\"</span>,<span class=\"string\">\"msg\"</span>:<span class=\"string\">\"retrying of unary invoker failed\"</span>,<span class=\"string\">\"target\"</span>:<span class=\"string\">\"endpoint://client-174bf993-5731-4b29-9b30-7e958ade79a4/10.10.107.191:2379\"</span>,<span class=\"string\">\"attempt\"</span>:0,<span class=\"string\">\"error\"</span>:<span class=\"string\">\"rpc error: code = Unknown desc = etcdserver: re-configuration failed due to not enough started members\"</span>&#125;</span><br></pre></td></tr></table></figure><br>问题原因: 在重置前etcd容器处于运转之中导致无法进行节点的重置操作;<br>解决办法: 停止所有的容器以及docker服务然后再执行节点的重置操作<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker stop $(docker ps -aq) &amp;&amp; systemctl stop docker</span><br></pre></td></tr></table></figure></p>\n<p><br/></p>\n<h3 id=\"问题7-节点初始化在进行preflight时候提示-error-execution-phase-preflight-ERROR-ImagePull\"><a href=\"#问题7-节点初始化在进行preflight时候提示-error-execution-phase-preflight-ERROR-ImagePull\" class=\"headerlink\" title=\"问题7.节点初始化在进行preflight时候提示`error execution phase preflight:[ERROR ImagePull]\"></a>问题7.节点初始化在进行preflight时候提示`error execution phase preflight:[ERROR ImagePull]</h3><p>问题描述:<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">kubeadm init --config=kubeadm-config.yaml --upload-certs</span><br><span class=\"line\">[init] Using Kubernetes version: v1.18.4</span><br><span class=\"line\">[preflight] You can also perform this action <span class=\"keyword\">in</span> beforehand using <span class=\"string\">'kubeadm config images pull'</span></span><br><span class=\"line\">error execution phase preflight: [preflight] Some fatal errors occurred:</span><br><span class=\"line\">        [ERROR ImagePull]: failed to pull image registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v1.18.4: output: Error response from daemon: manifest <span class=\"keyword\">for</span> registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v1.18.4 not found: manifest unknown: manifest unknown</span><br><span class=\"line\">, error: <span class=\"built_in\">exit</span> status 1</span><br></pre></td></tr></table></figure><br>问题原因: 由于k8s.gcr.io官方镜像网站无法下载镜像，而采用的同步镜像源站<code>registry.cn-hangzhou.aliyuncs.com/google_containers/</code>仓库中没有指定k8s版本的依赖组件；<br>解决办法: 换其它镜像进行尝试或者离线将镜像包导入的docker中(参考前面的笔记<code>2-Kubernetes入门手动安装部署</code>)，建议在进行执行上面的命令前先执行<code>kubeadm config images pull --image-repository mirrorgcrio --kubernetes-version=1.18.4</code>查看镜像是否能被拉取;<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 常规k8s.gcr.io镜像站点</span></span><br><span class=\"line\"><span class=\"comment\"># gcr.azk8s.cn/google_containers/ # 已失效</span></span><br><span class=\"line\">registry.aliyuncs.com/google_containers/</span><br><span class=\"line\">registry.cn-hangzhou.aliyuncs.com/google_containers/</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># harbor中k8s.gcr.io的镜像</span></span><br><span class=\"line\">mirrorgcrio</span><br></pre></td></tr></table></figure></p>\n<p><br/></p>\n<h3 id=\"问题8-容器内部Kubernetes-Service不能ping\"><a href=\"#问题8-容器内部Kubernetes-Service不能ping\" class=\"headerlink\" title=\"问题8.容器内部Kubernetes Service不能ping;\"></a>问题8.容器内部Kubernetes Service不能ping;</h3><p>问题描述:<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">PING gateway-example.example.svc.cluster.local (10.105.141.232) 56(84) bytes of data.</span><br><span class=\"line\">From 172.17.76.171 (172.17.76.171) icmp_seq=1 Time to live exceeded</span><br><span class=\"line\">From 172.17.76.171 (172.17.76.171) icmp_seq=2 Time to live exceeded</span><br></pre></td></tr></table></figure><br>问题原因:在 Kubernetes 的网络中Service 就是 ping 不通的,因为 Kubernetes 只是为 Service 生成了一个虚拟 IP 地址，实现的方式有三种 <code>User space / Iptables / IPVS</code> 等代理模式;<br>不管是哪种代理模式Kubernetes Service 的 IP 背后都没有任何实体可以响应「ICMP」<code>全称为 Internet 控制报文协议（Internet Control Message Protocol）</code>,但是可以通过curl或者telnet进行访问与<br>问题解决:<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl cluster-info</span><br><span class=\"line\"><span class=\"comment\"># Kubernetes master is running at https://k8s.weiyigeek.top:6443</span></span><br><span class=\"line\"><span class=\"comment\"># KubeDNS is running at https://k8s.weiyigeek.top:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy</span></span><br></pre></td></tr></table></figure></p>\n<p><br></p>\n<h3 id=\"问题9-kubeadm-init初始化k8s集群时显示-ERROR-Swap-running-with-swap-on-is-not-supported-Please-disable-swap\"><a href=\"#问题9-kubeadm-init初始化k8s集群时显示-ERROR-Swap-running-with-swap-on-is-not-supported-Please-disable-swap\" class=\"headerlink\" title=\"问题9.kubeadm init初始化k8s集群时显示[ERROR Swap]: running with swap on is not supported. Please disable swap`\"></a>问题9.kubeadm init初始化k8s集群时显示<code></code>[ERROR Swap]: running with swap on is not supported. Please disable swap`</h3><p>错误信息:<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ sudo kubeadm init --config=/home/weiyigeek/k8s-init/kubeadm-init-config.yaml --upload-certs | tee kubeadm_init.log</span><br><span class=\"line\">[init] Using Kubernetes version: v1.19.6</span><br><span class=\"line\">[preflight] Running pre-flight checks</span><br><span class=\"line\">error execution phase preflight: [preflight] Some fatal errors occurred:</span><br><span class=\"line\">        [ERROR Swap]: running with swap on is not supported. Please <span class=\"built_in\">disable</span> swap</span><br><span class=\"line\">[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`</span><br><span class=\"line\">To see the stack trace of this error execute with --v=5 or higher</span><br></pre></td></tr></table></figure><br>问题原因: 由于加入节点的机器未禁用swap分区导致<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">weiyigeek@weiyigeek-107:~$ free</span><br><span class=\"line\">              total        used        free      shared  buff/cache   available</span><br><span class=\"line\">Mem:        8151908      299900     7270588         956      581420     7600492</span><br><span class=\"line\">Swap:       4194300           0     4194300</span><br></pre></td></tr></table></figure><br>解决版本: 禁用swap分区<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo swapoff -a &amp;&amp; sudo sed -i <span class=\"string\">'/ swap / s/^\\(.*\\)$/#\\1/g'</span> /etc/fstab &amp;&amp; free  <span class=\"comment\"># CentOS </span></span><br><span class=\"line\">sudo swapoff -a &amp;&amp; sudo sed -i <span class=\"string\">'s/^\\/swap.img\\(.*\\)$/#\\/swap.img \\1/g'</span> /etc/fstab &amp;&amp; free  <span class=\"comment\"># Ubuntu</span></span><br><span class=\"line\">              total        used        free      shared  buff/cache   available</span><br><span class=\"line\">Mem:        8151908      304428     7260196         956      587284     7595204</span><br><span class=\"line\">Swap:             0           0           0</span><br></pre></td></tr></table></figure></p>\n<p><br></p>\n<h3 id=\"问题10-kubeadm-初始化问题之coredns的STATUS为Pending\"><a href=\"#问题10-kubeadm-初始化问题之coredns的STATUS为Pending\" class=\"headerlink\" title=\"问题10.kubeadm 初始化问题之coredns的STATUS为Pending\"></a>问题10.kubeadm 初始化问题之coredns的STATUS为Pending</h3><ul>\n<li>环境说明: OS:Ubuntu-20.04 / K8s:1.19.3 / docker:19.03.13 / flannel:v0.13.0</li>\n<li>错误信息:<code>0/1 nodes are available: 1 node(s) had taint {node.kubernetes.io/not-ready: }, that the pod didn&#39;t tolerate.</code></li>\n<li>错误原因: kubeadm 初始化完成后未安装 flannel 网络插件</li>\n<li>解决流程：安装部署 flannel 网络插件<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl get pod --all-namespaces</span><br><span class=\"line\">  <span class=\"comment\"># NAMESPACE     NAME                                  READY   STATUS    RESTARTS   AGE</span></span><br><span class=\"line\">  <span class=\"comment\"># kube-system   coredns-6c76c8bb89-8cgjz              0/1     Pending   0          99s</span></span><br><span class=\"line\">  <span class=\"comment\"># kube-system   coredns-6c76c8bb89-wgbs9              0/1     Pending   0          99s</span></span><br><span class=\"line\"></span><br><span class=\"line\">$ kubectl describe pod -n kube-system coredns-6c76c8bb89-8cgjz</span><br><span class=\"line\">...</span><br><span class=\"line\">  <span class=\"comment\"># Events:</span></span><br><span class=\"line\">  <span class=\"comment\">#   Type     Reason            Age                From               Message</span></span><br><span class=\"line\">  <span class=\"comment\">#   ----     ------            ----               ----               -------</span></span><br><span class=\"line\">  <span class=\"comment\">#   Warning  FailedScheduling  39s (x2 over 39s)  default-scheduler  0/1 nodes are available: 1 node(s) had taint &#123;node.kubernetes.io/not-ready: &#125;, that the pod didn't tolerate.</span></span><br><span class=\"line\"></span><br><span class=\"line\">$ kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml</span><br><span class=\"line\">  <span class=\"comment\"># podsecuritypolicy.policy/psp.flannel.unprivileged created</span></span><br><span class=\"line\">  <span class=\"comment\"># clusterrole.rbac.authorization.k8s.io/flannel created</span></span><br><span class=\"line\">  <span class=\"comment\"># clusterrolebinding.rbac.authorization.k8s.io/flannel created</span></span><br><span class=\"line\">  <span class=\"comment\"># serviceaccount/flannel created</span></span><br><span class=\"line\">  <span class=\"comment\"># configmap/kube-flannel-cfg created</span></span><br><span class=\"line\">  <span class=\"comment\"># daemonset.apps/kube-flannel-ds created</span></span><br><span class=\"line\"></span><br><span class=\"line\">$ kubectl get pod --all-namespaces</span><br><span class=\"line\">  <span class=\"comment\"># NAMESPACE     NAME                                  READY   STATUS    RESTARTS   AGE</span></span><br><span class=\"line\">  <span class=\"comment\"># kube-system   coredns-6c76c8bb89-8cgjz              1/1     Running   0          5m12s</span></span><br><span class=\"line\">  <span class=\"comment\"># kube-system   coredns-6c76c8bb89-wgbs9              1/1     Running   0          5m12s</span></span><br><span class=\"line\"></span><br><span class=\"line\">$ kubectl get node</span><br><span class=\"line\">  <span class=\"comment\"># NAME          STATUS   ROLES    AGE   VERSION</span></span><br><span class=\"line\">  <span class=\"comment\"># ubuntu   Ready    master   30m   v1.19.3</span></span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p><br/></p>\n<h3 id=\"问题11-kubeadm-初始化问题之coredns的STATUS为ContainerCreating\"><a href=\"#问题11-kubeadm-初始化问题之coredns的STATUS为ContainerCreating\" class=\"headerlink\" title=\"问题11.kubeadm 初始化问题之coredns的STATUS为ContainerCreating\"></a>问题11.kubeadm 初始化问题之coredns的STATUS为ContainerCreating</h3><ul>\n<li>环境说明: OS:Ubuntu-20.04 / K8s:1.19.3 / docker:19.03.13 / flannel:v0.13.0</li>\n<li>错误信息:<code>rpc error: code = Unknown desc = [failed to set up sandbox container &quot;355.....4ec7&quot; network for pod &quot;coredns-&quot;: networkPlugin cni failed to set up pod &quot;coredns-6c76c8bb89-6xgjl_kube-system&quot;</code></li>\n<li>错误原因: kubeadm 初始化CNI网络插件有误</li>\n<li>解决流程：重新进行Kubeadm初始化即可并且验证serviceSubnet是否为10.96.0.0/12;<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 资源信息</span></span><br><span class=\"line\">weiyigeek@ubuntu:~$ kubectl get pod -n kube-system</span><br><span class=\"line\">NAME                                  READY   STATUS              RESTARTS   AGE</span><br><span class=\"line\">coredns-6c76c8bb89-87zh7              0/1     ContainerCreating   0          18h</span><br><span class=\"line\">coredns-6c76c8bb89-p68x8              0/1     ContainerCreating   0          18h</span><br><span class=\"line\">etcd-ubuntu                      1/1     Running             0          18h</span><br><span class=\"line\">kube-apiserver-ubuntu            1/1     Running             0          18h</span><br><span class=\"line\">kube-controller-manager-ubuntu   1/1     Running             0          18h</span><br><span class=\"line\">kube-proxy-22t2f                      1/1     Running             0          17h</span><br><span class=\"line\">kube-proxy-wcjrv                      1/1     Running             0          18h</span><br><span class=\"line\">kube-scheduler-ubuntu            1/1     Running             0          18h</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 删除重新够构建</span></span><br><span class=\"line\">weiyigeek@ubuntu:~$ kubectl delete pod -n kube-system coredns-6c76c8bb89-87zh7 coredns-6c76c8bb89-p68x8</span><br><span class=\"line\">pod <span class=\"string\">\"coredns-6c76c8bb89-87zh7\"</span> deleted</span><br><span class=\"line\">pod <span class=\"string\">\"coredns-6c76c8bb89-p68x8\"</span> deleted</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p><br/></p>\n<h3 id=\"问题12-k8s-Cluster-IP-无法连接报错dial-tcp-10-96-0-1-443-connect-no-route-to-host\"><a href=\"#问题12-k8s-Cluster-IP-无法连接报错dial-tcp-10-96-0-1-443-connect-no-route-to-host\" class=\"headerlink\" title=\"问题12.k8s Cluster IP 无法连接报错dial tcp 10.96.0.1:443: connect: no route to host\"></a>问题12.k8s Cluster IP 无法连接报错<code>dial tcp 10.96.0.1:443: connect: no route to host</code></h3><p>报错信息：<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">dial tcp 10.96.0.1:443: i/o timeout</span><br><span class=\"line\">dial tcp 10.96.0.1:443: connect: no route to host</span><br></pre></td></tr></table></figure><br>报错原因:</p>\n<ul>\n<li>coredns Pod 未正常启动</li>\n<li>calico 网络插件未安装 或 calico-kube-controllers Pod 未正常启动<br>解决办法: 查看对应的报错信息进行解决;<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">~$ kubectl get pod -n kube-system | grep -e <span class=\"string\">\"calico|coredns\"</span></span><br><span class=\"line\"></span><br><span class=\"line\">~$ curl http://10.96.0.1:443</span><br><span class=\"line\">Client sent an HTTP request to an HTTPS server.</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p><br></p>\n<h3 id=\"问题13-kubeadm-init-执行初始化节点时显示ERROR-Swap与-WARNING-IsDockerSystemdCheck\"><a href=\"#问题13-kubeadm-init-执行初始化节点时显示ERROR-Swap与-WARNING-IsDockerSystemdCheck\" class=\"headerlink\" title=\"问题13.kubeadm init 执行初始化节点时显示ERROR Swap与 WARNING IsDockerSystemdCheck\"></a>问题13.kubeadm init 执行初始化节点时显示ERROR Swap与 WARNING IsDockerSystemdCheck</h3><p>错误信息:<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo kubeadm init</span><br><span class=\"line\">[init] Using Kubernetes version: v1.21.0</span><br><span class=\"line\">[preflight] Running pre-flight checks</span><br><span class=\"line\">  [WARNING IsDockerSystemdCheck]: detected <span class=\"string\">\"cgroupfs\"</span> as the Docker cgroup driver. The recommended driver is <span class=\"string\">\"systemd\"</span>. Please follow the guide at https://kubernetes.io/docs/setup/cri/</span><br><span class=\"line\">error execution phase preflight: [preflight] Some fatal errors occurred:</span><br><span class=\"line\">  [ERROR Swap]: running with swap on is not supported. Please <span class=\"built_in\">disable</span> swap</span><br><span class=\"line\">[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`</span><br><span class=\"line\">To see the stack trace of this error execute with --v=5 or higher</span><br></pre></td></tr></table></figure><br>报错原因: ERROR Swap 是由于当前操作系统未关闭swap交换分区,而WARNING IsDockerSystemdCheck警告则说明cgroup driver未采用systemd。</p>\n<p>解决办法:<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 1.关闭Swapp交换分区</span></span><br><span class=\"line\">sudo swapoff -a &amp;&amp; sudo sed -i <span class=\"string\">'/ swap / s/^\\(.*\\)$/#\\1/g'</span> /etc/fstab &amp;&amp; free  <span class=\"comment\"># CentOS</span></span><br><span class=\"line\">sudo swapoff -a &amp;&amp; sudo sed -i <span class=\"string\">'s/^\\/swap.img\\(.*\\)$/#\\/swap.img \\1/g'</span> /etc/fstab &amp;&amp; free  <span class=\"comment\">#Ubuntu</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 2.更改docker的 cgroup driver 驱动为systemd</span></span><br><span class=\"line\">cat /etc/docker/daemon.json</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  <span class=\"string\">\"registry-mirrors\"</span>: [</span><br><span class=\"line\">     <span class=\"string\">\"https://registry.cn-hangzhou.aliyuncs.com\"</span></span><br><span class=\"line\">  ],</span><br><span class=\"line\">  <span class=\"string\">\"max-concurrent-downloads\"</span>: 10,</span><br><span class=\"line\">  <span class=\"string\">\"log-driver\"</span>: <span class=\"string\">\"json-file\"</span>,</span><br><span class=\"line\">  <span class=\"string\">\"log-level\"</span>: <span class=\"string\">\"warn\"</span>,</span><br><span class=\"line\">  <span class=\"string\">\"log-opts\"</span>: &#123;</span><br><span class=\"line\">    <span class=\"string\">\"max-size\"</span>: <span class=\"string\">\"10m\"</span>,</span><br><span class=\"line\">    <span class=\"string\">\"max-file\"</span>: <span class=\"string\">\"3\"</span></span><br><span class=\"line\">    &#125;,</span><br><span class=\"line\">  <span class=\"string\">\"exec-opts\"</span>: [<span class=\"string\">\"native.cgroupdriver=systemd\"</span>],</span><br><span class=\"line\">  <span class=\"string\">\"storage-driver\"</span>: <span class=\"string\">\"overlay2\"</span>,</span><br><span class=\"line\">  <span class=\"string\">\"insecure-registries\"</span>: [<span class=\"string\">\"harbor.weiyigeek\"</span>, <span class=\"string\">\"harbor.weiyi\"</span>, <span class=\"string\">\"harbor.cloud\"</span>],</span><br><span class=\"line\">  <span class=\"string\">\"data-root\"</span>:<span class=\"string\">\"/home/data/docker/\"</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p><br></p>\n<h3 id=\"问题14-k8s-master-依赖镜像无法拉取\"><a href=\"#问题14-k8s-master-依赖镜像无法拉取\" class=\"headerlink\" title=\"问题14.k8s master 依赖镜像无法拉取\"></a>问题14.k8s master 依赖镜像无法拉取</h3><p>错误信息:<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Error response from daemon: manifest <span class=\"keyword\">for</span> registry.cn-hangzhou.aliyuncs.com/google_containers/coredns/coredns:v1.8.0 not found: manifest unknown: manifest unknown</span><br><span class=\"line\">Error response from daemon: No such image: registry.cn-hangzhou.aliyuncs.com/google_containers/coredns/coredns:v1.8.0</span><br><span class=\"line\">Error: No such image: registry.cn-hangzhou.aliyuncs.com/google_containers/coredns/coredns:v1.8.0</span><br></pre></td></tr></table></figure><br>解决办法: 上docker的hub平台上搜索拉取后然后更改tag即可，地址:<a href=\"https://hub.docker.com/。\" target=\"_blank\" rel=\"noopener\">https://hub.docker.com/。</a><br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker pull coredns/coredns:1.8.0</span><br><span class=\"line\">docker tag coredns/coredns:1.8.0 registry.cn-hangzhou.aliyuncs.com/google_containers/coredns/coredns:v1.8.0</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"问题15-Pod-一直处于-Pending-状态\"><a href=\"#问题15-Pod-一直处于-Pending-状态\" class=\"headerlink\" title=\"问题15.Pod 一直处于 Pending 状态\"></a>问题15.Pod 一直处于 Pending 状态</h3><p>描述: Pending 说明 Pod 还没有调度到某个 Node 上面。可以通过 <code>kubectl describe pod &lt;pod-name&gt;</code> 命令查看到当前 Pod 的事件，进而判断为什么没有调度.</p>\n<ul>\n<li>错误信息: <figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl describe pod mypod</span><br><span class=\"line\">...</span><br><span class=\"line\">Events:</span><br><span class=\"line\">  Type     Reason            Age                From               Message</span><br><span class=\"line\">  ----     ------            ----               ----               -------</span><br><span class=\"line\">  Warning  FailedScheduling  12s (x6 over 27s)  default-scheduler  0/4 nodes are available: 2 Insufficient cpu.</span><br></pre></td></tr></table></figure></li>\n<li>可能原因: <figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">- 资源不足，集群内所有的 Node 都不满足该 Pod 请求的 CPU、内存、GPU 或者临时存储空间等资源。解决方法是删除集群内不用的 Pod 或者增加新的 Node。</span><br><span class=\"line\">- HostPort 端口已被占用，通常推荐使用 Service 对外开放服务端口</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p><br></p>\n<h3 id=\"问题16-Pod-一直处于-Waiting-或-ContainerCreating-状态\"><a href=\"#问题16-Pod-一直处于-Waiting-或-ContainerCreating-状态\" class=\"headerlink\" title=\"问题16.Pod 一直处于 Waiting 或 ContainerCreating 状态\"></a>问题16.Pod 一直处于 Waiting 或 ContainerCreating 状态</h3><p>描述: 首先还是通过 kubectl describe pod <pod-name> 命令查看到当前 Pod 的事件</p>\n<ul>\n<li>错误信息: <figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl -n kube-system describe pod nginx-pod</span><br><span class=\"line\">  <span class=\"comment\"># Events:</span></span><br><span class=\"line\">  <span class=\"comment\">#   Type     Reason                 Age               From               Message</span></span><br><span class=\"line\">  <span class=\"comment\">#   ----     ------                 ----              ----               -------</span></span><br><span class=\"line\">  <span class=\"comment\">#   Normal   Scheduled              1m                default-scheduler  Successfully assigned nginx-pod to node1</span></span><br><span class=\"line\">  <span class=\"comment\">#   Normal   SuccessfulMountVolume  1m                kubelet, gpu13     MountVolume.SetUp succeeded for volume \"config-volume\"</span></span><br><span class=\"line\">  <span class=\"comment\">#   Normal   SuccessfulMountVolume  1m                kubelet, gpu13     MountVolume.SetUp succeeded for volume \"coredns-token-sxdmc\"</span></span><br><span class=\"line\">  <span class=\"comment\">#   Warning  FailedSync             2s (x4 over 46s)  kubelet, gpu13     Error syncing pod</span></span><br><span class=\"line\">  <span class=\"comment\">#   Normal   SandboxChanged         1s (x4 over 46s)  kubelet, gpu13     Pod sandbox changed, it will be killed and re-created.</span></span><br></pre></td></tr></table></figure></li>\n<li>问题原因:  发现是 cni0 网桥配置了一个不同网段的 IP 地址导致，删除该网桥（网络插件会自动重新创建）即可修复<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 可以发现，该 Pod 的 Sandbox 容器无法正常启动，具体原因需要查看 Kubelet 日志：</span></span><br><span class=\"line\">$ journalctl -u kubelet</span><br><span class=\"line\">...</span><br><span class=\"line\">Mar 14 04:22:04 node1 kubelet[29801]: E0314 04:22:04.649912   29801 cni.go:294] Error adding network: failed to <span class=\"built_in\">set</span> bridge addr: <span class=\"string\">\"cni0\"</span> already has an IP address different from 10.244.4.1/24</span><br><span class=\"line\">Mar 14 04:22:04 node1 kubelet[29801]: E0314 04:22:04.649941   29801 cni.go:243] Error <span class=\"keyword\">while</span> adding to cni network: failed to <span class=\"built_in\">set</span> bridge addr: <span class=\"string\">\"cni0\"</span> already has an IP address different from 10.244.4.1/24</span><br><span class=\"line\">Mar 14 04:22:04 node1 kubelet[29801]: W0314 04:22:04.891337   29801 cni.go:258] CNI failed to retrieve network namespace path: Cannot find network namespace <span class=\"keyword\">for</span> the terminated container <span class=\"string\">\"c4fd616cde0e7052c240173541b8543f746e75c17744872aa04fe06f52b5141c\"</span></span><br><span class=\"line\">Mar 14 04:22:05 node1 kubelet[29801]: E0314 04:22:05.965801   29801 remote_runtime.go:91] RunPodSandbox from runtime service failed: rpc error: code = 2 desc = NetworkPlugin cni failed to <span class=\"built_in\">set</span> up pod <span class=\"string\">\"nginx-pod\"</span> network: failed to <span class=\"built_in\">set</span> bridge addr: <span class=\"string\">\"cni0\"</span> already has an IP address different from 10.244.4.1/24</span><br></pre></td></tr></table></figure></li>\n<li>解决办法：<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ ip link <span class=\"built_in\">set</span> cni0 down</span><br><span class=\"line\">$ brctl delbr cni0</span><br></pre></td></tr></table></figure></li>\n<li>其它原因:<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">镜像拉取失败，比如</span><br><span class=\"line\">    配置了错误的镜像</span><br><span class=\"line\">    Kubelet 无法访问镜像（国内环境访问 gcr.io 需要特殊处理）</span><br><span class=\"line\">    私有镜像的密钥配置错误</span><br><span class=\"line\">    镜像太大，拉取超时（可以适当调整 kubelet 的 --image-pull-progress-deadline 和 --runtime-request-timeout 选项）</span><br><span class=\"line\">CNI 网络错误，一般需要检查 CNI 网络插件的配置，比如</span><br><span class=\"line\">    无法配置 Pod 网络</span><br><span class=\"line\">    无法分配 IP 地址</span><br><span class=\"line\">容器无法启动，需要检查是否打包了正确的镜像或者是否配置了正确的容器参数</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p><br></p>\n<h3 id=\"问题17-Pod-处于-ImagePullBackOff-状态\"><a href=\"#问题17-Pod-处于-ImagePullBackOff-状态\" class=\"headerlink\" title=\"问题17.Pod 处于 ImagePullBackOff 状态\"></a>问题17.Pod 处于 ImagePullBackOff 状态</h3><p>描述: 这通常是镜像名称配置错误或者私有镜像的密钥配置错误导致。这种情况可以使用 <code>docker pull &lt;image&gt;</code> 来验证镜像是否可以正常拉取。</p>\n<ul>\n<li>错误信息：<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl describe pod mypod</span><br><span class=\"line\">...</span><br><span class=\"line\">Events:</span><br><span class=\"line\">  Type     Reason                 Age                From                                Message</span><br><span class=\"line\">  ----     ------                 ----               ----                                -------</span><br><span class=\"line\">  Normal   Scheduled              36s                default-scheduler                  </span><br><span class=\"line\">  Normal   Pulling                17s (x2 over 33s)  kubelet, k8s-agentpool1-38622806-0  pulling image <span class=\"string\">\"a1pine\"</span></span><br><span class=\"line\">  Warning  Failed                 14s (x2 over 29s)  kubelet, k8s-agentpool1-38622806-0  Failed to pull image <span class=\"string\">\"a1pine\"</span>: rpc error: code = Unknown desc = Error response from daemon: repository a1pine not found: does not exist or no pull access</span><br><span class=\"line\">  Warning  Failed                 14s (x2 over 29s)  kubelet, k8s-agentpool1-38622806-0  Error: ErrImagePull</span><br><span class=\"line\">  Normal   SandboxChanged         4s (x7 over 28s)   kubelet, k8s-agentpool1-38622806-0  Pod sandbox changed, it will be killed and re-created.</span><br><span class=\"line\">  Normal   BackOff                4s (x5 over 25s)   kubelet, k8s-agentpool1-38622806-0  Back-off pulling image <span class=\"string\">\"a1pine\"</span></span><br><span class=\"line\">  Warning  Failed                 1s (x6 over 25s)   kubelet, k8s-agentpool1-38622806-0  Error: ImagePullBackOff</span><br></pre></td></tr></table></figure></li>\n<li>解决办法:<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 1. 如果是私有镜像，需要首先创建一个 docker-registry 类型的 Secret</span></span><br><span class=\"line\">kubectl create secret docker-registry my-secret --docker-server=DOCKER_REGISTRY_SERVER --docker-username=DOCKER_USER --docker-password=DOCKER_PASSWORD --docker-email=DOCKER_EMAIL</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 2. 然后在容器中引用这个 Secret</span></span><br><span class=\"line\">spec:</span><br><span class=\"line\">  containers:</span><br><span class=\"line\">  - name: private-reg-container</span><br><span class=\"line\">    image: &lt;your-private-image&gt;</span><br><span class=\"line\">  imagePullSecrets:</span><br><span class=\"line\">  - name: my-secret</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p><br></p>\n<h3 id=\"问题18-Pod-一直处于-CrashLoopBackOff-状态\"><a href=\"#问题18-Pod-一直处于-CrashLoopBackOff-状态\" class=\"headerlink\" title=\"问题18.Pod 一直处于 CrashLoopBackOff 状态\"></a>问题18.Pod 一直处于 CrashLoopBackOff 状态</h3><p>描述: CrashLoopBackOff 状态说明容器曾经启动了，但又异常退出了。此时 Pod 的 RestartCounts 通常是大于 0 的，可以先查看一下容器的日志</p>\n<ul>\n<li>问题原因: <figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">* 容器进程退出</span><br><span class=\"line\">* 健康检查失败退出</span><br><span class=\"line\">* OOMKilled</span><br><span class=\"line\">$ kubectl describe pod mypod</span><br><span class=\"line\">...</span><br><span class=\"line\">Containers:</span><br><span class=\"line\">  sh:</span><br><span class=\"line\">    Container ID:  docker://3f7a2ee0e7e0e16c22090a25f9b6e42b5c06ec049405bc34d3aa183060eb4906</span><br><span class=\"line\">    Image:         alpine</span><br><span class=\"line\">    Image ID:      docker-pullable://alpine@sha256:7b848083f93822dd21b0a2f14a110bd99f6efb4b838d499df6d04a49d0debf8b</span><br><span class=\"line\">    Port:          &lt;none&gt;</span><br><span class=\"line\">    Host Port:     &lt;none&gt;</span><br><span class=\"line\">    State:          Terminated</span><br><span class=\"line\">      Reason:       OOMKilled</span><br><span class=\"line\">      Exit Code:    2</span><br><span class=\"line\">    Last State:     Terminated</span><br><span class=\"line\">      Reason:       OOMKilled</span><br><span class=\"line\">      Exit Code:    2</span><br><span class=\"line\">    Ready:          False</span><br><span class=\"line\">    Restart Count:  3</span><br><span class=\"line\">    Limits:</span><br><span class=\"line\">      cpu:     1</span><br><span class=\"line\">      memory:  1G</span><br><span class=\"line\">    Requests:</span><br><span class=\"line\">      cpu:        100m</span><br><span class=\"line\">      memory:     500M</span><br><span class=\"line\">...</span><br><span class=\"line\">* 如果此时如果还未发现线索，还可以到容器内执行命令来进一步查看退出原因</span><br><span class=\"line\">  kubectl <span class=\"built_in\">exec</span> cassandra -- cat /var/<span class=\"built_in\">log</span>/cassandra/system.log</span><br><span class=\"line\">*  如果还是没有线索，那就需要 SSH 登录该 Pod 所在的 Node 上，查看 Kubelet 或者 Docker 的日志进一步排查了</span><br><span class=\"line\"><span class=\"comment\"># Query Node</span></span><br><span class=\"line\">kubectl get pod &lt;pod-name&gt; -o wide</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"comment\"># SSH to Node</span></span><br><span class=\"line\">ssh &lt;username&gt;@&lt;node-name&gt;</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p><br></p>\n<h3 id=\"问题-19-Pod-处于-Error-状态\"><a href=\"#问题-19-Pod-处于-Error-状态\" class=\"headerlink\" title=\"问题 19.Pod 处于 Error 状态\"></a>问题 19.Pod 处于 Error 状态</h3><p>通常处于 Error 状态说明 Pod 启动过程中发生了错误。常见的原因包括</p>\n<pre><code>依赖的 ConfigMap、Secret 或者 PV 等不存在\n\n请求的资源超过了管理员设置的限制，比如超过了 LimitRange 等\n\n违反集群的安全策略，比如违反了 PodSecurityPolicy 等\n\n容器无权操作集群内的资源，比如开启 RBAC 后，需要为 ServiceAccount 配置角色绑定\n</code></pre><p><br></p>\n<h3 id=\"问题-20-Pod-处于-Terminating-或-Unknown-状态\"><a href=\"#问题-20-Pod-处于-Terminating-或-Unknown-状态\" class=\"headerlink\" title=\"问题 20.Pod 处于 Terminating 或 Unknown 状态\"></a>问题 20.Pod 处于 Terminating 或 Unknown 状态</h3><p>从 v1.5 开始，Kubernetes 不会因为 Node 失联而删除其上正在运行的 Pod，而是将其标记为 Terminating 或 Unknown 状态。想要删除这些状态的 Pod 有三种方法：</p>\n<pre><code>从集群中删除该 Node。使用公有云时，kube-controller-manager 会在 VM 删除后自动删除对应的 Node。而在物理机部署的集群中，需要管理员手动删除 Node（如 kubectl delete node &lt;node-name&gt;。\n\nNode 恢复正常。Kubelet 会重新跟 kube-apiserver 通信确认这些 Pod 的期待状态，进而再决定删除或者继续运行这些 Pod。\n\n用户强制删除。用户可以执行 kubectl delete pods &lt;pod&gt; --grace-period=0 --force 强制删除 Pod。除非明确知道 Pod 的确处于停止状态（比如 Node 所在 VM 或物理机已经关机），否则不建议使用该方法。特别是 StatefulSet 管理的 Pod，强制删除容易导致脑裂或者数据丢失等问题。\n</code></pre><p> 如果 Kubelet 是以 Docker 容器的形式运行的，此时 kubelet 日志中可能会发现如下的错误：<br> <figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;<span class=\"string\">\"log\"</span>:<span class=\"string\">\"E0926 19:59:39.977461   54420 nestedpendingoperations.go:262] Operation for \\\"\\\\\\\"kubernetes.io/secret/30f3ffec-a29f-11e7-b693-246e9607517c-default-token-6tpnm\\\\\\\" (\\\\\\\"30f3ffec-a29f-11e7-b693-246e9607517c\\\\\\\")\\\" failed. No retries permitted until 2017-09-26 19:59:41.977419403 +0800 CST (durationBeforeRetry 2s). Error: UnmountVolume.TearDown failed for volume \\\"default-token-6tpnm\\\" (UniqueName: \\\"kubernetes.io/secret/30f3ffec-a29f-11e7-b693-246e9607517c-default-token-6tpnm\\\") pod \\\"30f3ffec-a29f-11e7-b693-246e9607517c\\\" (UID: \\\"30f3ffec-a29f-11e7-b693-246e9607517c\\\") : remove /var/lib/kubelet/pods/30f3ffec-a29f-11e7-b693-246e9607517c/volumes/kubernetes.io~secret/default-token-6tpnm: device or resource busy\\n\"</span>,<span class=\"string\">\"stream\"</span>:<span class=\"string\">\"stderr\"</span>,<span class=\"string\">\"time\"</span>:<span class=\"string\">\"2017-09-26T11:59:39.977728079Z\"</span>&#125;</span><br><span class=\"line\">&#123;<span class=\"string\">\"log\"</span>:<span class=\"string\">\"E0926 19:59:39.977461   54420 nestedpendingoperations.go:262] Operation for \\\"\\\\\\\"kubernetes.io/secret/30f3ffec-a29f-11e7-b693-246e9607517c-default-token-6tpnm\\\\\\\" (\\\\\\\"30f3ffec-a29f-11e7-b693-246e9607517c\\\\\\\")\\\" failed. No retries permitted until 2017-09-26 19:59:41.977419403 +0800 CST (durationBeforeRetry 2s). Error: UnmountVolume.TearDown failed for volume \\\"default-token-6tpnm\\\" (UniqueName: \\\"kubernetes.io/secret/30f3ffec-a29f-11e7-b693-246e9607517c-default-token-6tpnm\\\") pod \\\"30f3ffec-a29f-11e7-b693-246e9607517c\\\" (UID: \\\"30f3ffec-a29f-11e7-b693-246e9607517c\\\") : remove /var/lib/kubelet/pods/30f3ffec-a29f-11e7-b693-246e9607517c/volumes/kubernetes.io~secret/default-token-6tpnm: device or resource busy\\n\"</span>,<span class=\"string\">\"stream\"</span>:<span class=\"string\">\"stderr\"</span>,<span class=\"string\">\"time\"</span>:<span class=\"string\">\"2017-09-26T11:59:39.977728079Z\"</span>&#125;</span><br></pre></td></tr></table></figure><br>如果是这种情况，则需要给 kubelet 容器设置 –containerized 参数并传入以下的存储卷<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 以使用 calico 网络插件为例</span></span><br><span class=\"line\">      -v /:/rootfs:ro,shared \\</span><br><span class=\"line\">      -v /sys:/sys:ro \\</span><br><span class=\"line\">      -v /dev:/dev:rw \\</span><br><span class=\"line\">      -v /var/<span class=\"built_in\">log</span>:/var/<span class=\"built_in\">log</span>:rw \\</span><br><span class=\"line\">      -v /run/calico/:/run/calico/:rw \\</span><br><span class=\"line\">      -v /run/docker/:/run/docker/:rw \\</span><br><span class=\"line\">      -v /run/docker.sock:/run/docker.sock:rw \\</span><br><span class=\"line\">      -v /usr/lib/os-release:/etc/os-release \\</span><br><span class=\"line\">      -v /usr/share/ca-certificates/:/etc/ssl/certs \\</span><br><span class=\"line\">      -v /var/lib/docker/:/var/lib/docker:rw,shared \\</span><br><span class=\"line\">      -v /var/lib/kubelet/:/var/lib/kubelet:rw,shared \\</span><br><span class=\"line\">      -v /etc/kubernetes/ssl/:/etc/kubernetes/ssl/ \\</span><br><span class=\"line\">      -v /etc/kubernetes/config/:/etc/kubernetes/config/ \\</span><br><span class=\"line\">      -v /etc/cni/net.d/:/etc/cni/net.d/ \\</span><br><span class=\"line\">      -v /opt/cni/bin/:/opt/cni/bin/ \\</span><br></pre></td></tr></table></figure><br>处于 Terminating 状态的 Pod 在 Kubelet 恢复正常运行后一般会自动删除。但有时也会出现无法删除的情况，并且通过 kubectl delete pods <pod> –grace-period=0 –force 也无法强制删除。此时一般是由于 finalizers 导致的，通过 kubectl edit 将 finalizers 删除即可解决。<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">\"finalizers\"</span>: [</span><br><span class=\"line\">  <span class=\"string\">\"foregroundDeletion\"</span></span><br><span class=\"line\">]</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"问题-21-修改静态-Pod-的-Manifest-后未自动重建\"><a href=\"#问题-21-修改静态-Pod-的-Manifest-后未自动重建\" class=\"headerlink\" title=\"问题 21.修改静态 Pod 的 Manifest 后未自动重建\"></a>问题 21.修改静态 Pod 的 Manifest 后未自动重建</h3><p>Kubelet 使用 inotify 机制检测 /etc/kubernetes/manifests 目录（可通过 Kubelet 的 –pod-manifest-path 选项指定）中静态 Pod 的变化，并在文件发生变化后重新创建相应的 Pod。但有时也会发生修改静态 Pod 的 Manifest 后未自动创建新 Pod 的情景，此时一个简单的修复方法是重启 Kubelet。</p>\n<h3 id=\"问题-22-Namespace-一直处于-terminating-状态\"><a href=\"#问题-22-Namespace-一直处于-terminating-状态\" class=\"headerlink\" title=\"问题 22.Namespace 一直处于 terminating 状态\"></a>问题 22.Namespace 一直处于 terminating 状态</h3><p>Namespace 一直处于 terminating 状态，一般有两种原因：</p>\n<pre><code>Namespace 中还有资源正在删除中\n\nNamespace 的 Finalizer 未正常清理\n</code></pre><p>对第一个问题，可以执行下面的命令来查询所有的资源<br>kubectl api-resources –verbs=list –namespaced -o name | xargs -n 1 kubectl get –show-kind –ignore-not-found -n $NAMESPACE</p>\n<p>而第二个问题则需要手动清理 Namespace 的 Finalizer 列表：<br> kubectl get namespaces $NAMESPACE -o json | jq ‘.spec.finalizers=[]’ &gt; /tmp/ns.json<br>kubectl proxy &amp;<br>curl -k -H “Content-Type: application/json” -X PUT –data-binary @/tmp/ns.json <a href=\"http://127.0.0.1:8001/api/v1/namespaces/$NAMESPACE/finalize\" target=\"_blank\" rel=\"noopener\">http://127.0.0.1:8001/api/v1/namespaces/$NAMESPACE/finalize</a></p>\n<h3 id=\"Pod\"><a href=\"#Pod\" class=\"headerlink\" title=\"Pod\"></a>Pod</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> Warning  Failed            2m19s (x4 over 3m4s)  kubelet            Error: failed to create containerd task: OCI runtime create failed: container_linux.go:380: starting container process caused: process_linux.go:545: container init caused: process_linux.go:508: setting cgroup config <span class=\"keyword\">for</span> procHooks process caused: failed to write <span class=\"string\">\"107374182400000\"</span>: write /sys/fs/cgroup/cpu,cpuacct/system.slice/containerd.service/kubepods-burstable-pod6e586bca_1fd9_412a_892c_a77b38d7f3ec.slice:cri-containerd:app/cpu.cfs_quota_us: invalid argument: unknown</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">resources:</span><br><span class=\"line\">           requests:</span><br><span class=\"line\">             memory: <span class=\"string\">\"512Mi\"</span></span><br><span class=\"line\">             cpu: <span class=\"string\">\"100m\"</span></span><br><span class=\"line\">           limits:</span><br><span class=\"line\">             memory: <span class=\"string\">\"2048Mi\"</span></span><br><span class=\"line\">             cpu: <span class=\"string\">\"1000m\"</span></span><br><span class=\"line\">cpu 没有该Gi单位</span><br></pre></td></tr></table></figure>\n<p><br></p>\n<h2 id=\"问题记录\"><a href=\"#问题记录\" class=\"headerlink\" title=\"问题记录\"></a>问题记录</h2><ul>\n<li><strong>问题1.MountVolume.SetUp failed for volume “default-token-zglkd” : failed to sync secret cache: timed out waiting for the condition</strong><br>问题复现:<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">~/K8s/Day8/demo7$ kubectl get pod</span><br><span class=\"line\">NAME             READY   STATUS              RESTARTS   AGE</span><br><span class=\"line\">web-pvc-demo-0   0/1     ContainerCreating   0          58s</span><br><span class=\"line\"></span><br><span class=\"line\">~/K8s/Day8/demo7$ kubectl describe pod web-pvc-demo-0</span><br><span class=\"line\">Events:</span><br><span class=\"line\">  Type     Reason       Age    From               Message</span><br><span class=\"line\">  ----     ------       ----   ----               -------</span><br><span class=\"line\">  Normal   Scheduled    2m11s  default-scheduler  Successfully assigned default/web-pvc-demo-0 to k8s-node-5</span><br><span class=\"line\">  Warning  FailedMount  2m11s  kubelet            MountVolume.SetUp failed <span class=\"keyword\">for</span> volume <span class=\"string\">\"default-token-zglkd\"</span> : failed to sync secret cache: timed out waiting <span class=\"keyword\">for</span> the condition</span><br><span class=\"line\">  Warning  FailedMount  9s     kubelet            Unable to attach or mount volumes: unmounted volumes=[diskpv], unattached volumes=[diskpv default-token-zglkd]: timed out waiting <span class=\"keyword\">for</span> the condition</span><br></pre></td></tr></table></figure>\n问题原因: 由于kubernetes的MountVolume有一定的缓存导致已删除绑定的PV不可再重复的挂载;<br>解决办法: 删除无法挂载的PV卷以及PVC卷，如果还是不能解决直接重启集群;</li>\n</ul>\n<hr>\n<ul>\n<li><strong>问题2.使用NFS动态提供Kubernetes存储卷在创建PVC后一直是pending状态, 显示正在等待由外部供应器“fuseim.pri/ifs”或由系统管理员手动创建的卷</strong><br>问题环境: k8s(v1.23.1)<br>问题复现: 通过<code>kubectl describe</code>命令查看错误提示信息 <code>waiting for a volume to be created, either by external provisioner “fuseim.pri/ifs” or manually created by system administrator。</code>其次是通过<br><code>kubectl logs</code> 命令查看 <code>nfs-client-provisioner</code> pod日志中有<code>unexpected error getting claim reference: selfLink was empty, can’t make reference</code> 提示。<br>问题原因: 在v1.16版本将在ObjectMeta和ListMeta对象中弃用SelfLink字段，并且在v1.20版本之后默认禁用了selfLink(但是我们仍然可以通过参数的形式来进行恢复)<br>问题解决: 在 k8s 的 master 端 找到 kube-apiserver.yaml 文件，并在文件中的command参数中添加 <code>- --feature-gates=RemoveSelfLink=false</code>或者在其systemd单元服务中加入此参数然后重启即可。<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/etc/kubernetes/manifests/kube-apiserver.yaml</span><br><span class=\"line\">- --feature-gates=RemoveSelfLink=<span class=\"literal\">false</span></span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<ul>\n<li><strong>问题3.Kubelet启动异常报Image garbage collection failed once. Stats initialization may not have completed yet: failed to get imageFs info: unable to find data in memory cache错误</strong><br>问题复现:<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">E0704 15:20:03.875017    7912 kubelet.go:1292] Image garbage collection failed once. Stats initialization may not have completed yet: failed to get imageFs info: unable to find data <span class=\"keyword\">in</span> memory cache</span><br><span class=\"line\">E0704 15:20:03.920105    7912 kubelet.go:1853] skipping pod synchronization - [container runtime status check may not have completed yet, PLEG is not healthy: pleg has yet to be successful]</span><br></pre></td></tr></table></figure>\n解决办法: 由于我们<code>cgroupdriver</code>使用了<code>systemd</code> 因此我们需要升级systemd.<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">yum update -y</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p>在k8s集群中加入额外主机报<code>failure loading certificate for CA: couldn‘t load the certificate file</code>错误。</p>\n<p>错误信息: 当k8s做集群高可用的时候，需要将另一个master加入到当前master出现了如下错误。<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">failure loading certificate <span class=\"keyword\">for</span> CA: couldn<span class=\"string\">'t load the certificate file /etc/kubernetes/pki/ca.crt: open /etc/kubernetes/pki/ca.crt: no such file or directory</span></span><br></pre></td></tr></table></figure></p>\n<p>问题原因: 由于新的节点上没有kubernetes集群上的pki目录中的ca证书。<br>解决办法:<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">scp -rp /etc/kubernetes/pki/ca.* master02:/etc/kubernetes/pki</span><br><span class=\"line\">scp -rp /etc/kubernetes/pki/sa.* master02:/etc/kubernetes/pki</span><br><span class=\"line\">scp -rp /etc/kubernetes/pki/front-proxy-ca.* master02:/etc/kubernetes/pki</span><br><span class=\"line\">scp -rp /etc/kubernetes/pki/etcd/ca.* master02:/etc/kubernetes/pki/etcd</span><br><span class=\"line\">scp -rp /etc/kubernetes/admin.conf master02:/etc/kubernetes</span><br></pre></td></tr></table></figure></p>\n<hr>\n<h3 id=\"0x03-FAQ\"><a href=\"#0x03-FAQ\" class=\"headerlink\" title=\"0x03 FAQ\"></a>0x03 FAQ</h3><ul>\n<li><p><a href=\"https://www.cnblogs.com/dwq-good/p/13175150.html\" target=\"_blank\" rel=\"noopener\">https://www.cnblogs.com/dwq-good/p/13175150.html</a></p>\n</li>\n<li><p><a href=\"https://www.cnblogs.com/klvchen/archive/2018/09/04/9585746.html\" target=\"_blank\" rel=\"noopener\">https://www.cnblogs.com/klvchen/archive/2018/09/04/9585746.html</a></p>\n</li>\n<li><p><a href=\"https://blog.csdn.net/weixin_39963973/article/details/80568498\" target=\"_blank\" rel=\"noopener\">https://blog.csdn.net/weixin_39963973/article/details/80568498</a></p>\n</li>\n<li><p><a href=\"https://blog.csdn.net/weixin_43224068/article/details/104060320\" target=\"_blank\" rel=\"noopener\">https://blog.csdn.net/weixin_43224068/article/details/104060320</a></p>\n</li>\n<li><p><a href=\"https://www.cnblogs.com/djoker/p/10700607.html\" target=\"_blank\" rel=\"noopener\">https://www.cnblogs.com/djoker/p/10700607.html</a></p>\n</li>\n<li><p><a href=\"https://www.cnblogs.com/minseo/p/12447147.html\" target=\"_blank\" rel=\"noopener\">https://www.cnblogs.com/minseo/p/12447147.html</a></p>\n</li>\n<li><p><a href=\"https://www.cnblogs.com/1gaoyu/p/12983127.html\" target=\"_blank\" rel=\"noopener\">https://www.cnblogs.com/1gaoyu/p/12983127.html</a></p>\n</li>\n</ul>\n","comments":true,"excerpt":"[TOC]","categories":[{"name":"Containers","path":"api/categories/Containers.json"},{"name":"OperationTools","path":"api/categories/OperationTools.json"}],"tags":[{"name":"k8s","path":"api/tags/k8s.json"}]}