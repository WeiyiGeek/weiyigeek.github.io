{"title":"Ceph分布式存储文件系统基础学习与实践","slug":"系统运维/Application/DiskManagement/NetworkStorage/Ceph分布式存储文件系统基础学习与实践","date":"2021-07-22T01:35:30.000Z","updated":"2021-07-20T14:26:35.031Z","url":"2021/7-22-573.html","path":"api/articles/2021/7-22-573.html.json","covers":null,"content":"<p>[TOC]</p>\n<a id=\"more\"></a>\n<h2 id=\"0x00-前言简述\"><a href=\"#0x00-前言简述\" class=\"headerlink\" title=\"0x00 前言简述\"></a>0x00 前言简述</h2><h3 id=\"CEPH-简介\"><a href=\"#CEPH-简介\" class=\"headerlink\" title=\"CEPH 简介\"></a>CEPH 简介</h3><p><strong>Q: 什么是CEPH?</strong><br>答: Ceph是一个统一的分布式存储系统，设计初衷是提供较好的性能、可靠性和可扩展性。</p>\n<p>Ceph 项目最早起源于Sage就读博士期间的工作（最早的成果于2004年发表），并随后贡献给开源社区。在经过了数年的发展之后，目前已得到众多云计算厂商的支持并被广泛应用。RedHat及OpenStack都可与Ceph整合以支持虚拟机镜像的后端存储。</p>\n<p><br/></p>\n<p><strong>Ceph 特点:</strong></p>\n<ul>\n<li><p>1.高性能</p>\n<ul>\n<li>a. 摒弃了传统的集中式存储元数据寻址的方案，采用CRUSH算法，数据分布均衡，并行度高。</li>\n<li>b.考虑了容灾域的隔离，能够实现各类负载的副本放置规则，例如跨机房、机架感知等。</li>\n<li>c. 能够支持上千个存储节点的规模，支持TB到PB级的数据。</li>\n</ul>\n</li>\n<li><p>2.高可用</p>\n<ul>\n<li>a. 副本数可以灵活控制。</li>\n<li>b. 支持故障域分隔，数据强一致性。</li>\n<li>c. 多种故障场景自动进行修复自愈。</li>\n<li>d. 没有单点故障，自动管理。</li>\n</ul>\n</li>\n<li><p>3.高扩展</p>\n<ul>\n<li>a. 去中心化。</li>\n<li>b. 扩展灵活。</li>\n<li>c. 随着节点增加而线性增长。</li>\n</ul>\n</li>\n<li><p>4.特性丰富</p>\n<ul>\n<li>a. 支持三种存储接口：块存储、文件存储、对象存储。</li>\n<li>b. 支持自定义接口，支持多种语言驱动。</li>\n</ul>\n</li>\n</ul>\n<p><br/></p>\n<p><strong>Ceph 相关术语:</strong><br>描述: <a href=\"https://docs.ceph.com/en/latest/glossary/\" target=\"_blank\" rel=\"noopener\">https://docs.ceph.com/en/latest/glossary/</a></p>\n<ul>\n<li>Ceph Monitor (MON) : Ceph监控软件。</li>\n<li>Ceph Manager (MGR) : Ceph管理器软件，它在一个地方收集整个集群的所有状态。</li>\n<li>Ceph Metadata Server (MDS) : Ceph元数据软件。</li>\n<li>Ceph OSD 守护进程 : Ceph OSD 软件，与逻辑磁盘 ( OSD )交互。有时，Ceph 用户使用术语<code>“OSD”来指代“Ceph OSD 守护进程”，尽管正确的术语是“Ceph OSD”</code>。</li>\n<li>OSD ID : 定义 OSD 的整数。它由监视器生成，作为创建新 OSD 的一部分。</li>\n<li>OSD FSID : 这是一个唯一标识符，用于进一步提高 OSD 的唯一性，它位于 OSD 路径中名为osd_fsid 该 fsid术语可与uuid。</li>\n<li>OSD UUID : 就像 OSD fsid 一样，这是 OSD 唯一标识符，可与 fsid</li>\n<li>Bluestore : OSD BlueStore 是 OSD守护程序（kraken和更新版本）的新后端。与filestore不同，它直接将对象存储在Ceph块设备上，而不需要任何文件系统接口。</li>\n<li>Filestore : OSD 守护进程的后端，需要日志并将文件写入文件系统。</li>\n</ul>\n<ul>\n<li>Object (对象) ：有原生的API，而且也兼容Swift和S3的API。</li>\n<li>Block (块) ：支持精简配置、快照、克隆。</li>\n<li>File (文件系统) ：Posix接口，支持快照。</li>\n</ul>\n<ul>\n<li>Ceph 项目 ：Ceph的人员、软件、任务和基础设施的总称。</li>\n<li>Cephx : Ceph认证协议，Cephx的操作与Kerberos类似，但它没有单点故障。</li>\n<li>Ceph 系统 : Ceph 的两个或多个组件的集合。</li>\n<li>Ceph 节点 : Ceph 系统中的任何一台机器或服务器。</li>\n<li>Ceph 存储集群 : 存储用户数据的核心存储软件集（MON+OSD）。</li>\n<li>Ceph 集群映射 : 该组图包括监视器图、OSD 图、PG 图、MDS 图和 CRUSH 图。</li>\n<li>Ceph 对象存储 : 对象存储“产品”、服务或功能，主要由 Ceph 存储集群和 Ceph 对象网关组成。</li>\n<li>Ceph 对象网关 : Ceph 的 S3/Swift 网关组件(RGW)。</li>\n<li>Ceph 块设备 : Ceph 的块存储组件(RBD)。</li>\n<li>Ceph 块存储 : 与librbd管理程序（如 QEMU 或 Xen）和管理程序抽象层（如libvirt)。</li>\n<li>Ceph 文件系统 : Ceph 的 POSIX 文件系统组件。</li>\n<li>Ceph Pool : 池是用于存储对象的逻辑分区。</li>\n<li>LVM Lables : LVM 卷和组的可扩展元数据。它用于存储有关设备及其与 OSD 关系的 Ceph 特定信息。</li>\n</ul>\n<ul>\n<li>云平台 : 第三方云供应平台，如 OpenStack、CloudStack、OpenNebula、ProxMox 等。</li>\n<li>对象存储设备 : 物理或逻辑存储单元（例如 LUN）。有时 Ceph 用户使用术语“OSD”来指代Ceph OSD 守护进程，尽管正确的术语是“Ceph OSD”。</li>\n</ul>\n<p><br/></p>\n<h3 id=\"CEPH-组成及原理\"><a href=\"#CEPH-组成及原理\" class=\"headerlink\" title=\"CEPH 组成及原理\"></a>CEPH 组成及原理</h3><p>描述: 无论您是想向云平台提供 Ceph 对象存储和/或 Ceph 块设备服务、部署 Ceph 文件系统还是将 Ceph 用于其他目的，所有 Ceph 存储集群部署都从设置每个 Ceph 节点、您的网络和 Ceph 开始存储集群。</p>\n<p>Tips : 一个 Ceph 存储集群至少需要一个 <code>Ceph Monitor、Ceph Manager 和 Ceph OSD（对象存储守护进程）, 运行 Ceph 文件系统客户端时也需要 Ceph (MDS) 元数据服务器</code>。<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">MDSs  -- Monitors -- Managers -- OSDs</span><br></pre></td></tr></table></figure></p>\n<ul>\n<li><p>1.<code>Monitors</code>：Ceph Monitor ( ceph-mon ) 维护集群状态的映射，包括监视器映射、管理器映射、OSD 映射、MDS 映射和 CRUSH 映射。这些映射是 Ceph 守护进程相互协调所需的关键集群状态。监视器还负责管理守护进程和客户端之间的身份验证。通常至少需要三个监视器才能实现冗余和高可用性。</p>\n</li>\n<li><p>2.<code>Managers</code>: Ceph Managers 守护进程 ( ceph-mgr ) 负责跟踪运行时指标和 Ceph 集群的当前状态，包括存储利用率、当前性能指标和系统负载。Ceph 管理器守护进程还托管基于 Python 的模块来管理和公开 Ceph 集群信息，包括基于 Web 的Ceph 仪表板和 REST API, 高可用性通常至少需要两个管理器。</p>\n</li>\n<li><p>3.<code>Ceph OSD</code>: Ceph OSD（对象存储守护进程 ceph-osd）存储数据，处理数据复制、恢复、重新平衡，并通过检查其他 Ceph OSD 守护进程的心跳来向 Ceph 监视器和管理器提供一些监控信息。通常至少需要 3 个 Ceph OSD 来实现冗余和高可用性。</p>\n</li>\n<li><p>4.<code>MDS</code>：Ceph 元数据服务器（MDS，ceph-mds）代表Ceph 文件系统存储元数据（即 Ceph 块设备和 Ceph 对象存储不使用 MDS）。Ceph的元数据服务器允许POSIX文件系统的用户来执行基本的命令（如 ls，find没有放置在一个Ceph存储集群的巨大负担等等）。</p>\n</li>\n</ul>\n<p>Tips : Ceph 将数据作为对象存储在逻辑存储池中, 使用 CRUSH 算法 Ceph 计算出哪个归置组应该包含该对象，并进一步计算出哪个 Ceph OSD Daemon 应该存储该归置组，CRUSH 算法使 Ceph 存储集群能够动态扩展、重新平衡和恢复。</p>\n","comments":true,"excerpt":"[TOC]","categories":[{"name":"storage","path":"api/categories/storage.json"}],"tags":[{"name":"Ceph","path":"api/tags/Ceph.json"}]}